{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPKrM//IB6rUEqJ6PfgwtFW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JordanDCunha/Automated-Data-Collection-with-R-A-Practical-Guide-to-Web-Scraping-and-Text-Mining/blob/main/Chapter_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Case Study: UNESCO World Heritage Sites in Danger\n",
        "\n",
        "## Introduction\n",
        "\n",
        "In this case study, we explore how to scrape and analyze data from Wikipedia using **R**.  \n",
        "The goal is to examine the geographic distribution of UNESCO World Heritage Sites that are currently listed as *in danger*.\n",
        "\n",
        "The data source is the Wikipedia page:\n",
        "\n",
        "http://en.wikipedia.org/wiki/List_of_World_Heritage_in_Danger\n",
        "\n",
        "The table on this page contains:\n",
        "- Site name  \n",
        "- Location (including coordinates)  \n",
        "- Type of heritage (cultural or natural)  \n",
        "- Year of inscription  \n",
        "- Year of endangerment  \n",
        "\n",
        "We will:\n",
        "\n",
        "1. Load required packages.\n",
        "2. Scrape the HTML table from Wikipedia.\n",
        "3. Clean and prepare the data.\n",
        "4. Extract latitude and longitude using regular expressions.\n",
        "5. Plot the sites on a world map.\n",
        "\n",
        "This example demonstrates a core principle:\n",
        "\n",
        "> **Data are abundant ‚Äî retrieve them, prepare them, use them.**\n",
        "\n",
        "---\n",
        "\n",
        "## Load Required Libraries\n",
        "\n",
        "We use:\n",
        "- `stringr` for text manipulation\n",
        "- `XML` for HTML parsing\n",
        "- `maps` for visualization\n",
        "\n",
        "---\n",
        "\n",
        "## Scrape and Clean the Data\n",
        "\n",
        "We:\n",
        "- Parse the HTML page\n",
        "- Extract all tables\n",
        "- Select the relevant table\n",
        "- Rename variables\n",
        "- Convert years to numeric\n",
        "- Extract coordinates using regular expressions\n",
        "\n",
        "---\n",
        "\n",
        "## Visualize the Sites\n",
        "\n",
        "We:\n",
        "- Plot a world map\n",
        "- Add points for endangered sites\n",
        "- Use different symbols for cultural vs natural sites\n",
        "\n",
        "Cultural sites ‚Üí triangles  \n",
        "Natural sites ‚Üí dots  \n",
        "\n",
        "This allows us to visually inspect geographic clustering.\n",
        "\n",
        "---\n",
        "\n",
        "## Key Observations\n",
        "\n",
        "- Many endangered sites are located in Africa, the Middle East, and Southwest Asia.\n",
        "- Cultural sites appear clustered in the Middle East and Southwest Asia.\n",
        "- Natural sites are more prominent in Africa.\n",
        "- Many sites were listed as endangered shortly after inscription.\n",
        "\n",
        "This raises interesting political and institutional questions about UNESCO‚Äôs designation process.\n",
        "\n",
        "---\n",
        "\n",
        "Now let‚Äôs implement everything in R.\n"
      ],
      "metadata": {
        "id": "AyEeROwRJmos"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mwMRrx7CpBBP"
      },
      "outputs": [],
      "source": [
        "# Install packages if necessary\n",
        "# install.packages(c(\"stringr\", \"XML\", \"maps\"))\n",
        "\n",
        "library(stringr)\n",
        "library(XML)\n",
        "library(maps)\n",
        "\n",
        "# Scrape the Wikipedia page\n",
        "url <- \"http://en.wikipedia.org/wiki/List_of_World_Heritage_in_Danger\"\n",
        "heritage_parsed <- htmlParse(url)\n",
        "tables <- readHTMLTable(heritage_parsed, stringsAsFactors = FALSE)\n",
        "\n",
        "# Select the second table (current endangered sites)\n",
        "danger_table <- tables[[2]]\n",
        "\n",
        "# Rename relevant columns (adjust if necessary depending on Wikipedia structure)\n",
        "colnames(danger_table)[1:5] <- c(\"name\", \"locn\", \"crit\", \"y_ins\", \"y_end\")\n",
        "\n",
        "# Keep only relevant columns\n",
        "danger_table <- danger_table[, c(\"name\", \"locn\", \"crit\", \"y_ins\", \"y_end\")]\n",
        "\n",
        "# Recode cultural/natural\n",
        "danger_table$crit <- ifelse(grepl(\"Natural\", danger_table$crit), \"nat\", \"cult\")\n",
        "\n",
        "# Convert inscription year to numeric\n",
        "danger_table$y_ins <- as.numeric(str_extract(danger_table$y_ins, \"[[:digit:]]{4}\"))\n",
        "\n",
        "# Extract last 4-digit year from endangerment column\n",
        "danger_table$y_end <- as.numeric(str_extract(danger_table$y_end, \"[[:digit:]]{4}$\"))\n",
        "\n",
        "# Regular expressions for coordinates\n",
        "reg_y <- \"[/][ -]*[[:digit:]]*[.]*[[:digit:]]*[;]\"\n",
        "reg_x <- \"[;][ -]*[[:digit:]]*[.]*[[:digit:]]*\"\n",
        "\n",
        "# Extract latitude\n",
        "y_coords <- str_extract(danger_table$locn, reg_y)\n",
        "y_coords <- as.numeric(str_sub(y_coords, 3, -2))\n",
        "danger_table$y_coords <- y_coords\n",
        "\n",
        "# Extract longitude\n",
        "x_coords <- str_extract(danger_table$locn, reg_x)\n",
        "x_coords <- as.numeric(str_sub(x_coords, 3, -1))\n",
        "danger_table$x_coords <- x_coords\n",
        "\n",
        "# Remove messy location column\n",
        "danger_table$locn <- NULL\n",
        "\n",
        "# Basic inspection\n",
        "dim(danger_table)\n",
        "head(danger_table)\n",
        "\n",
        "# Plot map\n",
        "pch_vals <- ifelse(danger_table$crit == \"nat\", 19, 2)\n",
        "\n",
        "map(\"world\", col = \"darkgrey\", lwd = 0.5,\n",
        "    mar = c(0.1, 0.1, 0.1, 0.1))\n",
        "\n",
        "points(danger_table$x_coords,\n",
        "       danger_table$y_coords,\n",
        "       pch = pch_vals)\n",
        "\n",
        "box()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.2 Some Remarks on Web Data Quality\n",
        "\n",
        "## Introduction\n",
        "\n",
        "The previous example demonstrated how easily web data can be scraped and visualized.  \n",
        "However, before collecting large amounts of online data, it is essential to ask:\n",
        "\n",
        "- What type of data best answers the research question?\n",
        "- Is the data quality sufficient?\n",
        "- Could the information be systematically biased or flawed?\n",
        "\n",
        "This section highlights key considerations when working with web data.\n",
        "\n",
        "---\n",
        "\n",
        "## Origins of Web Data\n",
        "\n",
        "Web data may be:\n",
        "\n",
        "- **Firsthand data** (e.g., tweets, forum posts, reviews)\n",
        "- **Secondhand data** (copied from offline sources)\n",
        "- **Scraped data** (collected from other online platforms)\n",
        "\n",
        "Sometimes the original source cannot be traced.  \n",
        "Even so, web data can still be useful ‚Äî provided we apply critical evaluation.\n",
        "\n",
        "For example, Wikipedia‚Äôs accuracy has been widely debated.  \n",
        "Some studies suggest it is comparable to traditional encyclopedias,  \n",
        "while others report inconsistencies.  \n",
        "\n",
        "The lesson:\n",
        "\n",
        "> Cross-validation is essential for any secondary data source.\n",
        "\n",
        "Reputation alone does not prevent errors.\n",
        "\n",
        "---\n",
        "\n",
        "## Data Quality Depends on Purpose\n",
        "\n",
        "Data quality is not absolute ‚Äî it depends on the intended use.\n",
        "\n",
        "Example:\n",
        "\n",
        "- A random sample of tweets may be suitable for analyzing hashtag usage.\n",
        "- The same sample may be biased for predicting election outcomes if collected during a political convention.\n",
        "\n",
        "Thus, representativeness matters depending on the research objective.\n",
        "\n",
        "For factual data (e.g., capital cities, wildlife populations),  \n",
        "there are clearer standards for validation.\n",
        "\n",
        "---\n",
        "\n",
        "## Web Data vs Traditional Data Collection\n",
        "\n",
        "Consider measuring popularity of a new phone.\n",
        "\n",
        "Traditional method:\n",
        "- Conduct a survey\n",
        "- Ask respondents about preferences\n",
        "\n",
        "Potential issues:\n",
        "- Sampling bias\n",
        "- Poor question wording\n",
        "- Non-response bias\n",
        "\n",
        "Web-based alternative:\n",
        "- Analyze online sales rankings\n",
        "- Use product reviews as proxies\n",
        "\n",
        "Advantages:\n",
        "- Larger coverage\n",
        "- Behavioral data instead of self-reported preferences\n",
        "- Lower cost\n",
        "\n",
        "Challenges:\n",
        "- Platform bias\n",
        "- Coverage limitations\n",
        "- Comparability across product generations\n",
        "\n",
        "Choosing data sources often involves trade-offs:\n",
        "\n",
        "- Accuracy vs completeness\n",
        "- Coverage vs validity\n",
        "- Cost vs precision\n",
        "\n",
        "---\n",
        "\n",
        "## Five-Step Guide for Web Data Collection\n",
        "\n",
        "1. **Define the exact information needed.**\n",
        "   - Be specific where possible.\n",
        "\n",
        "2. **Identify potential web sources.**\n",
        "   - Direct or indirect indicators.\n",
        "   - Consider official sites, social media, commercial platforms.\n",
        "\n",
        "3. **Understand the data generation process.**\n",
        "   - Who created the data?\n",
        "   - When and why?\n",
        "   - Are there systematic gaps?\n",
        "\n",
        "4. **Balance advantages and disadvantages.**\n",
        "   - Availability and legality\n",
        "   - Collection cost\n",
        "   - Compatibility with existing research\n",
        "   - Possibility of validation\n",
        "\n",
        "5. **Make a documented decision.**\n",
        "   - Choose the most suitable source.\n",
        "   - If feasible, collect from multiple sources for validation.\n",
        "\n",
        "---\n",
        "\n",
        "## Key Takeaway\n",
        "\n",
        "Web data does not inherently have lower quality than traditional data.  \n",
        "However, it requires:\n",
        "\n",
        "- Careful validation\n",
        "- Awareness of biases\n",
        "- Clear alignment between research question and data source\n",
        "\n",
        "Ultimately:\n",
        "\n",
        "> Data quality depends on the user‚Äôs purpose.\n"
      ],
      "metadata": {
        "id": "ggFVmMokJzQ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple Framework for Evaluating Web Data Quality\n",
        "\n",
        "# Step 1: Define research question\n",
        "research_question <- \"What is the popularity of a new smartphone model?\"\n",
        "\n",
        "# Step 2: Identify potential data sources\n",
        "data_sources <- c(\n",
        "  \"Twitter posts\",\n",
        "  \"Online sales rankings\",\n",
        "  \"Customer reviews\",\n",
        "  \"Survey data\"\n",
        ")\n",
        "\n",
        "# Step 3: Evaluate each source\n",
        "evaluation <- data.frame(\n",
        "  Source = data_sources,\n",
        "  Representativeness = NA,\n",
        "  Coverage = NA,\n",
        "  Potential_Bias = NA,\n",
        "  Validation_Possible = NA,\n",
        "  stringsAsFactors = FALSE\n",
        ")\n",
        "\n",
        "evaluation\n",
        "\n",
        "# Step 4: Manually document reasoning after inspection\n",
        "evaluation$Representativeness <- c(\n",
        "  \"Low (event-driven bias)\",\n",
        "  \"Medium (platform-specific users)\",\n",
        "  \"Medium (self-selection bias)\",\n",
        "  \"High (if properly sampled)\"\n",
        ")\n",
        "\n",
        "evaluation$Coverage <- c(\n",
        "  \"High volume but noisy\",\n",
        "  \"Limited to platform\",\n",
        "  \"Limited to buyers\",\n",
        "  \"Depends on sample size\"\n",
        ")\n",
        "\n",
        "evaluation$Potential_Bias <- c(\n",
        "  \"Political/event bias\",\n",
        "  \"Platform sales bias\",\n",
        "  \"Extreme opinions overrepresented\",\n",
        "  \"Response bias\"\n",
        ")\n",
        "\n",
        "evaluation$Validation_Possible <- c(\n",
        "  \"Yes (cross-platform comparison)\",\n",
        "  \"Yes (compare multiple retailers)\",\n",
        "  \"Yes (compare with surveys)\",\n",
        "  \"Yes (replication)\"\n",
        ")\n",
        "\n",
        "evaluation\n"
      ],
      "metadata": {
        "id": "sh0cNe2rJzC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.3 Technologies for Disseminating, Extracting, and Storing Web Data\n",
        "\n",
        "Collecting web data is not always as simple as scraping an HTML table.\n",
        "Modern websites use complex structures, dynamic content, and multiple data formats.\n",
        "To effectively scrape and process web data in R, we need a basic understanding of three major technological pillars:\n",
        "\n",
        "1. Technologies for disseminating content  \n",
        "2. Technologies for information extraction  \n",
        "3. Technologies for data storage  \n",
        "\n",
        "This section provides a structured overview of each.\n",
        "\n",
        "---\n",
        "\n",
        "# 1Ô∏è‚É£ Technologies for Disseminating Content\n",
        "\n",
        "These technologies define **how data are delivered on the Web**.\n",
        "\n",
        "## HTML (Hypertext Markup Language)\n",
        "- Structures how information is displayed in browsers.\n",
        "- Data appear in tables, lists, text, links.\n",
        "- Scrapers must understand how data are stored in the underlying HTML code.\n",
        "- Parsed using HTML parsers.\n",
        "\n",
        "## XML (Extensible Markup Language)\n",
        "- Designed for storing and exchanging structured data.\n",
        "- Uses user-defined tags.\n",
        "- More flexible than HTML.\n",
        "- Requires XML parsers.\n",
        "\n",
        "## JSON (JavaScript Object Notation)\n",
        "- Lightweight data exchange format.\n",
        "- Frequently used by APIs (e.g., Twitter API).\n",
        "- Easy to parse in R.\n",
        "- Language-independent standard.\n",
        "\n",
        "## AJAX\n",
        "- Enables asynchronous loading of content.\n",
        "- Dynamically updates webpages without reloading.\n",
        "- Complicates scraping because data may not appear in static HTML.\n",
        "- Often requires browser tools or Selenium.\n",
        "\n",
        "## Plain Text\n",
        "- Unstructured data.\n",
        "- Requires pattern recognition techniques.\n",
        "- Processed using regular expressions or text mining.\n",
        "\n",
        "## HTTP (Hypertext Transfer Protocol)\n",
        "- The communication standard between client and server.\n",
        "- Most web scraping relies on HTTP requests.\n",
        "- Advanced scraping may require custom HTTP requests.\n",
        "\n",
        "---\n",
        "\n",
        "# 2Ô∏è‚É£ Technologies for Information Extraction\n",
        "\n",
        "Once documents are retrieved, we must extract relevant information.\n",
        "\n",
        "## XPath\n",
        "- Query language for navigating HTML/XML.\n",
        "- Selects specific nodes or elements.\n",
        "- Powerful for structured documents.\n",
        "\n",
        "## JSON Parsers\n",
        "- Automatically decode JSON objects into R structures.\n",
        "- No query language required.\n",
        "\n",
        "## Selenium\n",
        "- Browser automation framework.\n",
        "- Handles dynamic (AJAX-heavy) websites.\n",
        "- Simulates clicks and inputs.\n",
        "\n",
        "## Regular Expressions\n",
        "- Pattern-matching tools for extracting structured text.\n",
        "- Useful for numbers, names, coordinates.\n",
        "- Helpful when markup structure cannot be exploited.\n",
        "\n",
        "## Text Mining\n",
        "- Extracts latent patterns from unstructured text.\n",
        "- Enables classification and clustering.\n",
        "- Used for sentiment analysis, topic modeling, etc.\n",
        "\n",
        "---\n",
        "\n",
        "# 3Ô∏è‚É£ Technologies for Data Storage\n",
        "\n",
        "After extraction, data must be stored efficiently.\n",
        "\n",
        "## Databases & SQL\n",
        "- Reliable, scalable storage.\n",
        "- Support multi-user access.\n",
        "- Fast querying for large datasets.\n",
        "- Useful for large-scale scraping projects.\n",
        "\n",
        "## R Native Storage\n",
        "- CSV files\n",
        "- RDS files\n",
        "- Binary formats\n",
        "- Suitable for small to medium projects.\n",
        "\n",
        "---\n",
        "\n",
        "# Key Insight\n",
        "\n",
        "Web scraping requires understanding:\n",
        "\n",
        "- How data are delivered (HTML, JSON, AJAX)\n",
        "- How to extract them (XPath, regex, parsers)\n",
        "- How to store them (R files, databases)\n",
        "\n",
        "You do not need to be an expert in all technologies ‚Äî\n",
        "but you must understand the basics to build effective scrapers.\n",
        "\n",
        "> Web scraping is not just about downloading data ‚Äî\n",
        "> it is about understanding the full data pipeline.\n"
      ],
      "metadata": {
        "id": "zWcCpil3Kh9i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install packages if necessary\n",
        "# install.packages(c(\"httr\", \"xml2\", \"rvest\", \"jsonlite\", \"DBI\", \"RSQLite\", \"stringr\"))\n",
        "\n",
        "library(httr)      # HTTP communication\n",
        "library(xml2)      # XML/HTML parsing\n",
        "library(rvest)     # Web scraping tools\n",
        "library(jsonlite)  # JSON parsing\n",
        "library(stringr)   # Regular expressions\n",
        "library(DBI)       # Database interface\n",
        "library(RSQLite)   # SQLite database\n",
        "\n",
        "# ------------------------------\n",
        "# 1Ô∏è‚É£ Dissemination: HTTP Request\n",
        "# ------------------------------\n",
        "response <- GET(\"https://httpbin.org/get\")\n",
        "status_code(response)\n",
        "\n",
        "# ------------------------------\n",
        "# 2Ô∏è‚É£ Extraction: HTML Parsing\n",
        "# ------------------------------\n",
        "html_page <- read_html(\"https://example.com\")\n",
        "title_node <- html_element(html_page, \"title\")\n",
        "html_text(title_node)\n",
        "\n",
        "# ------------------------------\n",
        "# 3Ô∏è‚É£ Extraction: JSON Parsing\n",
        "# ------------------------------\n",
        "json_data <- fromJSON('{\"product\":\"phone\",\"price\":799}')\n",
        "json_data\n",
        "\n",
        "# ------------------------------\n",
        "# 4Ô∏è‚É£ Extraction: Regular Expression\n",
        "# ------------------------------\n",
        "text_sample <- \"The price is $799.\"\n",
        "str_extract(text_sample, \"[0-9]+\")\n",
        "\n",
        "# ------------------------------\n",
        "# 5Ô∏è‚É£ Storage: Database Example\n",
        "# ------------------------------\n",
        "con <- dbConnect(RSQLite::SQLite(), \":memory:\")\n",
        "dbWriteTable(con, \"products\", data.frame(name=\"phone\", price=799))\n",
        "dbReadTable(con, \"products\")\n",
        "\n",
        "dbDisconnect(con)\n"
      ],
      "metadata": {
        "id": "by0RqIUSKjhz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.4 Structure of the Book\n",
        "\n",
        "This book is written for readers with diverse backgrounds and goals.  \n",
        "Depending on your experience with R and web technologies, you may read it sequentially or selectively.\n",
        "\n",
        "---\n",
        "\n",
        "## Who Should Read What?\n",
        "\n",
        "### If you have basic R knowledge but little web experience:\n",
        "Follow the book in order to build a strong foundation.\n",
        "\n",
        "### If you already have text data:\n",
        "Start with:\n",
        "- Chapter 8: Regular Expressions and String Functions\n",
        "- Chapter 10: Statistical Text Processing\n",
        "\n",
        "### If you are mainly interested in web scraping:\n",
        "Focus on scraping chapters.\n",
        "You may skip Chapter 10 (text mining),  \n",
        "but Chapter 8 (text manipulation basics) is strongly recommended.\n",
        "\n",
        "### If you are a teacher:\n",
        "- Exercises are provided after most chapters in Parts I and II.\n",
        "- Partial solutions are available on the book‚Äôs website.\n",
        "- Exercises can be used for homework or exams.\n",
        "\n",
        "---\n",
        "\n",
        "# Overview of the Three Parts\n",
        "\n",
        "---\n",
        "\n",
        "# üìò Part I: A Primer on Web and Data Technologies\n",
        "\n",
        "This section introduces foundational technologies:\n",
        "\n",
        "- HTTP\n",
        "- HTML\n",
        "- XML\n",
        "- JSON\n",
        "- AJAX\n",
        "- SQL\n",
        "- XPath\n",
        "- Regular Expressions\n",
        "\n",
        "Goal:\n",
        "- Understand how the Web works.\n",
        "- Learn how data are structured and transmitted.\n",
        "- Build core technical skills needed for scraping.\n",
        "\n",
        "Includes:\n",
        "- Concept explanations\n",
        "- Practical exercises\n",
        "\n",
        "---\n",
        "\n",
        "# üõ† Part II: A Practical Toolbox for Web Scraping and Text Mining\n",
        "\n",
        "This section focuses on implementation.\n",
        "\n",
        "Core topics include:\n",
        "\n",
        "## Web Scraping Techniques\n",
        "- Regular expressions\n",
        "- XPath\n",
        "- APIs\n",
        "- Source-specific scraping methods\n",
        "- Legal and ethical considerations\n",
        "\n",
        "## Statistical Text Processing\n",
        "- Supervised text classification\n",
        "- Unsupervised methods\n",
        "- Extracting latent information\n",
        "\n",
        "## Data Project Management in R\n",
        "- File system organization\n",
        "- Efficient coding with loops\n",
        "- Automating scraping tasks\n",
        "- Scheduling recurring data collection\n",
        "\n",
        "Goal:\n",
        "Turn foundational knowledge into applied skills.\n",
        "\n",
        "---\n",
        "\n",
        "# üìä Part III: Case Studies\n",
        "\n",
        "This section provides real-world applications.\n",
        "\n",
        "Each case study includes:\n",
        "- A research motivation\n",
        "- Data collection procedures\n",
        "- Cleaning and preprocessing\n",
        "- Analysis\n",
        "- Discussion of pitfalls\n",
        "\n",
        "Additionally:\n",
        "- Summary tables of techniques used\n",
        "- Key R packages and functions\n",
        "- Practical workflow insights\n",
        "\n",
        "---\n",
        "\n",
        "# Key Takeaway\n",
        "\n",
        "The book moves from:\n",
        "\n",
        "1. Understanding web technologies  \n",
        "2. Applying scraping and text mining tools  \n",
        "3. Executing full real-world projects  \n",
        "\n",
        "You may follow the full journey or jump directly to the sections most relevant to your goals.\n",
        "\n",
        "> The structure supports both learning and practical application.\n"
      ],
      "metadata": {
        "id": "ujfO-zy3K66L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Representation of the Book Structure in R\n",
        "\n",
        "book_structure <- list(\n",
        "\n",
        "  Part_I = list(\n",
        "    Title = \"Primer on Web and Data Technologies\",\n",
        "    Topics = c(\"HTTP\", \"HTML\", \"XML\", \"JSON\", \"AJAX\", \"SQL\",\n",
        "               \"XPath\", \"Regular Expressions\"),\n",
        "    Goal = \"Understand foundational web technologies\"\n",
        "  ),\n",
        "\n",
        "  Part_II = list(\n",
        "    Title = \"Web Scraping and Text Mining Toolbox\",\n",
        "    Topics = c(\"Scraping Techniques\",\n",
        "               \"APIs\",\n",
        "               \"Legal/Ethical Issues\",\n",
        "               \"Supervised Text Classification\",\n",
        "               \"Unsupervised Text Mining\",\n",
        "               \"Workflow & Automation\"),\n",
        "    Goal = \"Apply scraping and text mining in practice\"\n",
        "  ),\n",
        "\n",
        "  Part_III = list(\n",
        "    Title = \"Case Studies\",\n",
        "    Topics = c(\"Real-world scraping applications\",\n",
        "               \"Data cleaning\",\n",
        "               \"Workflow management\",\n",
        "               \"Common pitfalls\"),\n",
        "    Goal = \"Integrate techniques into complete projects\"\n",
        "  )\n",
        ")\n",
        "\n",
        "book_structure\n"
      ],
      "metadata": {
        "id": "vX58RcKyK9B1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}